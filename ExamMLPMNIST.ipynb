{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1h3OEqRDni_IAim5P24CU3ZVL8ruqxmR5",
      "authorship_tag": "ABX9TyN2JVlIqUyCXP2zQzK619Tk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JeonJeongumggumg/PyTorch/blob/main/ExamMLPMNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Ghl4H7jsvLlL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "tPZ8dnl2vMnJ"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = datasets.MNIST(root='MNIST_data/',train = True, transform = transforms.ToTensor(),download = True)#학습데이터, 0~255까지 값을 0~1사이 값으로 변환\n",
        "test_dataset = datasets.MNIST(root='MNIST_data/',train = False, transform = transforms.ToTensor(),download = True)#테스트데이터"
      ],
      "metadata": {
        "id": "E5eyQGekvrLt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(train_dataset))\n",
        "\n",
        "train_dataset_size = int(len(train_dataset)*0.85)\n",
        "valid_dataset_size = int(len(train_dataset)*0.15)\n",
        "train_dataset, valid_dataset = random_split(train_dataset, [train_dataset_size, valid_dataset_size])\n",
        "\n",
        "print(len(train_dataset), len(valid_dataset), len(test_dataset))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rcdgeWoQwLIM",
        "outputId": "191a4dc5-b746-4ab4-e071-e0a922d8cce2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "60000\n",
            "51000 9000 10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MNISTModel(nn.Module):\n",
        "    def __init__(self): #아키텍쳐를 구성하는 다양한 계층을 정의.\n",
        "        super().__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc1 = nn.Linear(784, 256)\n",
        "        self.fc2 = nn.Linear(256, 10)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(p=0.3)\n",
        "\n",
        "    def forward(self, data):#입력으로 주어진 학습데이터에 대해 피드포워드 수행\n",
        "        data = self.flatten(data) #입력층\n",
        "        data = self.fc1(data) #은닉층\n",
        "        data = self.relu(data) #ReLu(비선형함수)\n",
        "        data = self.dropout(data)#Dropout 과적합방지\n",
        "        logits = self.fc2(data)# 출력층\n",
        "        return logits"
      ],
      "metadata": {
        "id": "u2gR-acKxV0Y"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 32\n",
        "#배치 데이터를 만들기 위해 DataLoader설정\n",
        "train_dataset_loader = DataLoader(dataset = train_dataset, batch_size = BATCH_SIZE, shuffle = True)\n",
        "valid_dataset_loader = DataLoader(dataset = valid_dataset, batch_size = BATCH_SIZE, shuffle = True)\n",
        "test_dataset_loader = DataLoader(dataset = test_dataset, batch_size = BATCH_SIZE, shuffle = True)"
      ],
      "metadata": {
        "id": "MK6R-AdPwvAz"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MNISTModel()\n",
        "\n",
        "loss_function = nn.CrossEntropyLoss() #CEL에는 손실함수에는 softmax함수를 포함(정답이 0~9> 다중클래스를 분류를 위함)\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = 1e-2)"
      ],
      "metadata": {
        "id": "2zZTS34xx46P"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model_train(dataloader, model, loss_function, optimizer):\n",
        "\n",
        "  model.train() #신경망을 학습모드로 전환\n",
        "\n",
        "  train_loss_sum = train_correct = train_total = 0\n",
        "  total_train_batch = len(dataloader)\n",
        "\n",
        "  for images, labels in dataloader: #images에는 MNIST이미지 라벨에는 0~9 정답숫자\n",
        "      x_train = images.view(-1, 28*28)#처음크기에서 784로 전환\n",
        "      y_train = labels\n",
        "\n",
        "      outputs = model(x_train) #입력데이터에 대한 예측값을 계산\n",
        "      loss = loss_function(outputs, y_train) #모델 예측값과 정답과의 오차인 손실함수를 계산\n",
        "\n",
        "      optimizer.zero_grad() #역전파 코드, 즉 학습이 진행됨에 따라 모델파라미터를 업뎃하면서 최적화\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      train_loss_sum+= loss.item()\n",
        "\n",
        "      train_total += y_train.size(0)\n",
        "      train_correct +=((torch.argmax(outputs, 1)==y_train)).sum().item()\n",
        "\n",
        "  train_avg_loss = train_loss_sum / total_train_batch #학습데이터 평균오차 계산\n",
        "  train_avg_accuracy = 100*train_correct / train_total #학습데이터 평균 정확도 계산\n",
        "\n",
        "\n",
        "  return (train_avg_loss, train_avg_accuracy)\n"
      ],
      "metadata": {
        "id": "ZEr90m380SMU"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model_evaluate(dataloader, model, loss_function):\n",
        "\n",
        "  model.eval() #신경망을 검증모드로 전환\n",
        "\n",
        "  with torch.no_grad(): #미분하지 않겠음=모델파라미터를 업뎃시키지 않음\n",
        "\n",
        "      val_loss_sum = 0\n",
        "      val_correct=0\n",
        "      val_total = 0\n",
        "\n",
        "      val_loss_sum = val_correct = vla_total = 0\n",
        "\n",
        "      total_val_batch = len(dataloader)\n",
        "\n",
        "      for images, labels in dataloader:\n",
        "\n",
        "          x_val = images.view(-1, 28*28)\n",
        "          y_val = labels\n",
        "\n",
        "          outputs = model(x_val)\n",
        "          loss = loss_function(outputs, y_val)\n",
        "\n",
        "          val_loss_sum += loss.item()\n",
        "\n",
        "          val_total += y_val.size(0)\n",
        "          val_correct += ((torch.argmax(outputs,1)==y_val)).sum().item()\n",
        "\n",
        "      val_avg_loss = val_loss_sum / total_val_batch #검증데이터 평균 오차계산\n",
        "      val_avg_accuracy = 100*val_correct / val_total #검증데이터 정확도 계\n",
        "\n",
        "      return (val_avg_loss, val_avg_accuracy)\n"
      ],
      "metadata": {
        "id": "z5YBgoM-2qzl"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "train mode에서 중요한 것\n",
        "\n",
        "신경망을 학습모드로 변경하고 데이터로더에서 이미지와 정답을 가져와서 벡터로 변경, 오차 구하고, 역전파시켜 모델파라미터를 최적화\n",
        "\n",
        "train과 evaluate mode 비교\n",
        "\n",
        "검증모드 = 모델파라미터를 업뎃시키지 않기에 미분이 불필요(torch.no_grad())\n",
        "\n",
        "train의 역전파코드가 val에는 없음 = 파라미터 업뎃하지않고 현재 신경망의 정확도와 오차만 알려줌"
      ],
      "metadata": {
        "id": "SGic5AMU4Di8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_loss_list = []\n",
        "train_accuracy_list = []\n",
        "\n",
        "val_loss_list = []\n",
        "val_accuracy_list = []\n",
        "\n",
        "EPOCHS = 20\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  #model train\n",
        "  train_avg_loss, train_avg_accuracy = model_train(train_dataset_loader, model, loss_function, optimizer)\n",
        "\n",
        "  train_loss_list.append(train_avg_loss)\n",
        "  train_accuracy_list.append(train_avg_accuracy)\n",
        "\n",
        "  #model evaluate\n",
        "\n",
        "  val_avg_loss, val_avg_accuracy = model_evaluate(valid_dataset_loader, model, loss_function) #optimizer를 쓸 필요가 없음(최적화 불필요), 쓸 경우 인수문제로 typeError가 발견됨\n",
        "\n",
        "  val_loss_list.append(val_avg_loss)\n",
        "  val_accuracy_list.append(val_avg_accuracy)"
      ],
      "metadata": {
        "id": "c6zwtM2e5RP_"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model_test(dataloader, model):\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad(): #test set으로 데이터를 다룰 때에는 gradient를 주면 안된다.\n",
        "\n",
        "        test_loss_sum = 0\n",
        "        test_correct=0\n",
        "        test_total = 0\n",
        "\n",
        "        total_test_batch = len(dataloader)\n",
        "\n",
        "        for images, labels in dataloader: # images에는 이미지, labels에는 0-9 숫자\n",
        "\n",
        "            # reshape input image into [batch_size by 784]\n",
        "            # label is not one-hot encoded\n",
        "            x_test = images.view(-1, 28 * 28) #처음 크기는 (batch_size, 1, 28, 28) / 이걸 (batch_size, 784)로 변환\n",
        "            y_test = labels\n",
        "\n",
        "            outputs = model(x_test)\n",
        "            loss = loss_function(outputs, y_test)\n",
        "\n",
        "            test_loss_sum += loss.item()\n",
        "\n",
        "            test_total += y_test.size(0)  # label 열 사이즈 같음\n",
        "            test_correct += ((torch.argmax(outputs, 1)==y_test)).sum().item() # 예측한 값과 일치한 값의 합\n",
        "\n",
        "        test_avg_loss = test_loss_sum / total_test_batch\n",
        "        test_avg_accuracy = 100*test_correct / test_total\n",
        "\n",
        "        print('accuracy:', test_avg_accuracy)\n",
        "        print('loss:', test_avg_loss)"
      ],
      "metadata": {
        "id": "oaS5tU3TAluK"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_test(test_dataset_loader, model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y58vfAv4DY6V",
        "outputId": "8b4ba097-1a6c-4db1-a544-80f7a76d9260"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy: 98.02\n",
            "loss: 0.0661373647549502\n"
          ]
        }
      ]
    }
  ]
}